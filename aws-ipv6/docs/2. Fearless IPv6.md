# IPv6

It's like having a service mesh, but without the bloat. Each pod (we'll talk more about k8s later on) have their own unique address and... things just talk to each other. Reachability is a breeze, debugging is as easy as it gets and logs get useful addresses that you can check against later on. The solution is also really scalable (not to say that service meshes couldn't scale. Maybe they can, but that's unlikely.)

Some applications may die, yes, but that's the risk I'm willing to take. You have had 27 years to implement this, do better.

## Things to consider when going to IPv6

The addresses become much harder to calculate in your head and subnet calculations might not be as easy as before using a simple sheet on your cubical's wall. Getting used to this is somewhat a journey, but `ipcalc` supports v6 addresses. But in practice I haven't worried too much about it. Just doing some allocations to subnets and letting the computer to deal with that have done the trick this far. (That might be because of [IPAM](./3.%20IPAM.md), but that's also why I suggest using it.)

## It's faster

Well, yes and no. Yes in practical sense, no in a sense that if we would have same setup in both IPv4 and IPv6, it likely wouldn't make a difference. But that's not the case.

With IPv4 you're going to use some sort of network translation whenever you're going out from your own network. In our case, we're not so heavy on per VPC egress, so we're going to save some money with [Egress VPC](./6.%20Egress.md). So whenever the traffic needs to go out from our own network, it goes first to transit gateway, sees where it needs to go next, hops to egress vpc private subnet, goes to public subnet, NAT gateway and then to internet. And then it takes the same route back to our instance in our application VPC. While this is in absolute term quite quick, with `ping -v4` and `ping -v6` to same domain (hosted on cloudfront) it took 4 times longer to use v4 (0.3ms compared to 1.2ms, so not really that significant in human scale, but something to think about in your usecase.) It also saves some costs as both NAT gateway and Transit gateway bills per GB processed.

Adding additional hops (like overlay network like Calico) on the server itself might increase the difference even more.
